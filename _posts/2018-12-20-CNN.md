---
author:
- Jinshuo Dong
bibliography:
- 'mybib.bib'
title: NN study notes
...

We need two theorems that help us get a neat exposition:

Theorem. *$$f:X\to Y$$, then $$Df(x)$$ for each $$x$$ is an object of the same
type as $$f$$ itself. Note that $$Df(x)$$ is linear. In particular, if
$$f:X\to{\mathbb{R}}$$, then $$Df(x):X\to {\mathbb{R}}$$ is actually in
$$X^* $$. When $$X^* $$ is identified with $$X$$, there can be gradient descent.*

Theorem. *Chain rule says that the derivative of a composed mapping is
the (linear transform) composition of linear approximations of the
components at corresponding points.*

For more on derivatives of abstract mappings, see $$\S 3$$, Chapter I of
*Differential and Riemannian Manifolds* by Serge Lang.

Consider the following $$n$$ layer neural network:

$$x_0 \mapsto\cdots\mapsto\sigma(A_{k-1} x_{k-2})=x_{k-1} \mapsto A_{k}x_{k-1} \mapsto \sigma(A_{k}x_{k-1}) = x_{k}\mapsto\cdots\mapsto x_n \mapsto l(x_n)$$

This shows explicitly the behavior of the $$k$$-th layer and the loss
function at the end. Our target is to compute
$$\frac{\partial l(x_n)}{\partial A_k}$$ for $$k=1,2,\ldots, n$$. A note for
notations: In the example of $$\frac{\partial l(x_n)}{\partial A_k}$$,
$$l(x_n)$$ indicates only the **quantity** of interest, not the function
dependence. The function dependence is implicitly determined by the
above composed mapping diagram. We save the operator $$D$$ for the
derivative of an abstract mapping, and $$\nabla$$ as a special case of $$D$$
when the abstract mapping is a real-valued function.

Let’s first deal with the single layer NN. The idea is that top layers
of a NN can be treated as an abstract loss function, and hence the
following result can be applied recursively from top to bottom. This
facilitates the computation of $$\frac{\partial l(x_n)}{\partial A_k}$$
and essentially is back propagation.

Theorem. *Consider single layer NN with abstract loss function $$N$$:*

$$x \mapsto Ax \mapsto \sigma(Ax) = z \mapsto N(z).$$

*What we are really interested in is the mapping*

$$A\mapsto Ax \mapsto \sigma(Ax)\mapsto N(\sigma(Ax))\in{\mathbb{R}}.$$

*We have*

$$\frac{\partial N(z)}{\partial A} = {\,\mathrm{diag}\,}\sigma'(Ax)[x\nabla N(z)]^T.$$

Linearize the individual mappings in
$$A\mapsto Ax \mapsto \sigma(Ax)\mapsto N(\sigma(Ax))\in{\mathbb{R}}$$:

$$\delta A\mapsto \delta A\cdot x, y\mapsto{\,\mathrm{diag}\,}\sigma'(Ax)y, y\mapsto \langle \nabla N(\sigma(Ax)), y\rangle.$$

Notice that $$z = \sigma(Ax)$$, so the composed linear mapping is:

$$\delta A\mapsto \langle \nabla N(z), {\,\mathrm{diag}\,}\sigma'(Ax)\cdot\delta A \cdot x\rangle.$$

Assuming $$\nabla N(z)$$ is a row vector, the well-known identity
$$\mathrm{tr}(AB) = \mathrm{tr}(BA)$$ yields:
\begin{align}
    \langle \nabla N(z), {\,\mathrm{diag}\,}\sigma'(Ax)\cdot\delta A \cdot x\rangle  & = \nabla N(z)\cdot {\,\mathrm{diag}\,}\sigma'(Ax)\cdot\delta A \cdot x \\\\\\
    & = \mathrm{tr}(\nabla N(z)\cdot {\,\mathrm{diag}\,}\sigma'(Ax)\cdot\delta A \cdot x) \\\\\\
    & = \mathrm{tr}(x\cdot\nabla N(z)\cdot {\,\mathrm{diag}\,}\sigma'(Ax)\cdot\delta A) \\\\\\
    & = \mathrm{tr}(\big[{\,\mathrm{diag}\,}\sigma'(Ax)(x\nabla N(z))^T\big]^T\delta A)
\end{align}

Let $$N_k$$ be the mapping $$x_k\mapsto l(x_n)$$. It is the cut-off starting
from the output of the $$k$$-th layer. In particular, $$N_0$$ is the whole
network, and $$N_n$$ is just the $$l$$ function.

Apply this theorem with $$N = N_{k}, x = x_{k-1}, A=A_k$$. Note that
$$z = x_k$$. We have

$$\frac{\partial l(x_n)}{\partial A_k} = \frac{\partial N_{k}(x_k)}{\partial A_k} =
     {\,\mathrm{diag}\,}\sigma'(A_kx_{k-1}) (x_{k-1} \nabla N_{k}(x_k))^T.$$

In order to compute $$\nabla N_{k}(x_k)$$, we expand out one more layer
$$N_{k}(x_k) = N_{k+1}(x_{k+1}) = N_{k+1}(\sigma(A_{k+1}x_k))$$ and
directly linearize it:

$$\nabla N_{k}(x_k) = \nabla N_{k+1}(x_{k+1}) {\,\mathrm{diag}\,}\sigma'(A_{k+1}x_k) A_{k+1}.$$

This is a (backward) recursive relation with the initial condition
$$\nabla N_{n}(x_n) = \nabla l(x_n).$$ Now we have backpropagation
algorithm. In order to compute $$\frac{\partial l(x_n)}{\partial A_k}$$,
we will need $$x_k$$ and $$\nabla N_{k}(x_k)$$. $$x_k$$ can be computed in the
forward pass. The hard part is $$\nabla N_{k}$$. The recursive relation
suggest first reaching $$x_n$$ and then compute backward all the
$$\nabla N_{k}(x_k)$$.

The intense computation is matrix-vector multiplication. Each $$A$$ is
applied to a column vector once in the forward pass, and applied to a
row vector once in the backward pass.

The exposition in Sasha’s lecture notes has the same spirit but messier
scalar notation.

#### A bit more on rigorousness

Recall that at the end of the proof of , we transposed
$$x\cdot\nabla N(z)\cdot {\,\mathrm{diag}\,}\sigma'(Ax)$$ so that the
transposed matrix has the same dimension as $$A$$, and claimed the result
as the gradient. It is natural but is it legitimate, even under the
examination from the strictest rigor police[^1]?

Here is a (partial) answer from the algebraic viewpoint. In order to do
gradient descent on $$L(X,Y)$$, we have to identify $$L(X,Y)^* $$ with
$$L(X,Y)$$. That is achieved by introducing an inner product on $$L(X,Y)$$:

$$\langle A, B \rangle = \mathrm{tr}(A^TB).$$

The full derivative
computed in the proof of ,
$$\delta A\mapsto \langle \nabla N(z), {\,\mathrm{diag}\,}\sigma'(Ax)\cdot\delta A \cdot x\rangle$$,
is an object in $$L(X,Y)^* $$.
$$x\cdot\nabla N(z)\cdot {\,\mathrm{diag}\,}\sigma'(Ax)$$ is an object in
$$L(Y,X)$$. They are linked by the natural isomorphism between $$L(X,Y)^* $$
and $$L(Y,X)$$. The final line of the proof of expresses the full
derivative as an inner product of the above form. This justifies the
operation of transposition.

[^1]: See Ali Rahimi’s talk at NIPS 2017.
